{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a65ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"D:\\FOT\\sem4\\fda\\project\\spam_detection\\dataset\\processed\\dataset.csv\", encoding='latin-1')\n",
    "df.columns = ['text', 'label']\n",
    "df['text'] = df['text'].astype(str)\n",
    "df = df.dropna(subset=['label'])\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize data\n",
    "max_length = 128\n",
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Simple tokenization function\n",
    "encoded_texts = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Split data\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize split data\n",
    "train_encodings = tokenizer(X_train_texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "test_encodings = tokenizer(X_test_texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(\n",
    "    train_encodings['input_ids'], \n",
    "    train_encodings['attention_mask'],\n",
    "    torch.tensor(y_train)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    test_encodings['input_ids'], \n",
    "    test_encodings['attention_mask'],\n",
    "    torch.tensor(y_test)\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Setup training parameters\n",
    "device = torch.device('cpu')  # Use CPU only\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop with live graph\n",
    "num_epochs = 1\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = num_epochs * steps_per_epoch\n",
    "\n",
    "# Create scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Initialize variables for tracking metrics\n",
    "train_losses = []\n",
    "f1_scores = []\n",
    "steps = []\n",
    "\n",
    "# Setup live plotting\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.set_title('Training Progress')\n",
    "ax.set_xlabel('Steps')\n",
    "ax.set_ylabel('Metrics')\n",
    "loss_line, = ax.plot([], [], 'r-', label='Training Loss')\n",
    "f1_line, = ax.plot([], [], 'b-', label='F1 Score')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show(block=False)\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Starting training...\")\n",
    "global_step = 0\n",
    "eval_frequency = max(steps_per_epoch // 10, 1)  # Evaluate every 10% of an epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        # Record metrics\n",
    "        train_losses.append(loss.item())\n",
    "        steps.append(global_step)\n",
    "        \n",
    "        # Evaluate occasionally\n",
    "        if global_step % eval_frequency == 0:\n",
    "            model.eval()\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for eval_batch in test_loader:\n",
    "                    eval_input_ids, eval_attention_mask, eval_labels = [b.to(device) for b in eval_batch]\n",
    "                    outputs = model(input_ids=eval_input_ids, attention_mask=eval_attention_mask)\n",
    "                    preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "                    all_preds.extend(preds)\n",
    "                    all_labels.extend(eval_labels.cpu().numpy())\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            current_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "            f1_scores.append(current_f1)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nStep {global_step}: Loss = {loss.item():.4f}, F1 Score = {current_f1:.4f}\")\n",
    "            \n",
    "            # Update plot\n",
    "            loss_line.set_data(steps, train_losses)\n",
    "            f1_line.set_data(steps[::eval_frequency], f1_scores)\n",
    "            ax.relim()\n",
    "            ax.autoscale_view()\n",
    "            fig.canvas.draw()\n",
    "            fig.canvas.flush_events()\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "        global_step += 1\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate final metrics\n",
    "final_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "print(f\"\\nFinal F1 Score: {final_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Save plot\n",
    "plt.ioff()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, train_losses, 'r-', label='Training Loss')\n",
    "plt.plot([steps[i] for i in range(0, len(steps), eval_frequency)], f1_scores, 'b-', label='F1 Score')\n",
    "plt.title('Training Progress')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Metrics')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('training_progress.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'], \n",
    "            yticklabels=['Ham', 'Spam'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"roberta_spam_model.pt\")\n",
    "print(\"Model saved as 'roberta_spam_model.pt'\")\n",
    "\n",
    "# Example prediction function\n",
    "def predict_text(texts):\n",
    "    model.eval()\n",
    "    encoded_inputs = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    input_ids = encoded_inputs['input_ids'].to(device)\n",
    "    attention_mask = encoded_inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "    return preds\n",
    "\n",
    "# Example predictions\n",
    "test_messages = [\n",
    "    \"Congratulations! You've won a free iPhone! Click here to claim your prize now!\",\n",
    "    \"Hi, can you please send me the meeting notes from yesterday? Thanks!\"\n",
    "]\n",
    "\n",
    "predictions = predict_text(test_messages)\n",
    "for msg, pred in zip(test_messages, predictions):\n",
    "    print(f\"Message: {msg[:50]}{'...' if len(msg) > 50 else ''}\")\n",
    "    print(f\"Prediction: {'Spam' if pred == 1 else 'Ham'}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
